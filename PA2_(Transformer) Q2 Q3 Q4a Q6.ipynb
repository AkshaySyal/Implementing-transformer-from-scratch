{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "MRCr0QFbMy2m",
   "metadata": {
    "id": "MRCr0QFbMy2m"
   },
   "source": [
    "This notebook has Q2, Q3, Q4a and Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cd103",
   "metadata": {
    "id": "3a4cd103"
   },
   "outputs": [],
   "source": [
    "# Author: Roi Yehoshua\n",
    "# Date: January 2024\n",
    "# MIT License\n",
    "\n",
    "# Based on the PyTorch implementation from https://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FQhn1tXC3pZR",
   "metadata": {
    "id": "FQhn1tXC3pZR"
   },
   "source": [
    "Q2 Model implementation: Fill in the missing code parts in the provided notebook adhering closely to the specifications in the paper. Pay close attention to the instructions and comments in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ko4E0JbHI5O",
   "metadata": {
    "id": "2ko4E0JbHI5O"
   },
   "outputs": [],
   "source": [
    "# To run this example make sure you have the following packages installed:\n",
    "%pip install spacy torchtext portalocker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GXxqHpCfHQO_",
   "metadata": {
    "id": "GXxqHpCfHQO_"
   },
   "outputs": [],
   "source": [
    "import portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5dee5",
   "metadata": {
    "id": "62a5dee5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17027ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b17027ed",
    "outputId": "8fc1fc7a-d39c-45fb-86fd-5a8ae1be9585"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)  # For reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad93f9",
   "metadata": {
    "id": "ffad93f9"
   },
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "$$\n",
    "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\\\\n",
    "    \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\  \n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf156177",
   "metadata": {
    "id": "bf156177"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"The multi-head attention module\"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure the dimension of the model is divisible by the number of heads.\n",
    "        # This is necessary to equally divide the embedding dimension across heads.\n",
    "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
    "\n",
    "        self.d_model = d_model           # Total dimension of the model\n",
    "        self.num_heads = num_heads       # Number of attention heads\n",
    "        self.d_k = d_model // num_heads  # Dimnsion of each head. We assume d_v = d_k\n",
    "\n",
    "        # Linear transformations for queries, keys, and values\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Final linear layer to project the concatenated heads' outputs back to d_model dimensions\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "       ### WRITE YOUR CODE HERE\n",
    "\n",
    "        # 1. Calculate attention scores with scaling\n",
    "        # 2. Apply mask (if provided) by setting masked positions to a large negative value\n",
    "        # 3. Apply softmax to attention scores to get probabilities\n",
    "        # 4. Return the weighted sum of values based on attention probabilities\n",
    "      d_k = Q.size(-1)\n",
    "      scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "      if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "      p_attn = scores.softmax(dim=-1)\n",
    "      output = torch.matmul(p_attn, V)\n",
    "      return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input tensor to [batch_size, num_heads, seq_length, d_k]\n",
    "        # to prepare for multi-head attention processing\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Inverse operation of split_heads: combine the head outputs back into the original tensor shape\n",
    "        # [batch_size, seq_length, d_model]\n",
    "        batch_size, num_heads, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "\n",
    "        # 1. Linearly project the queries, keys, and values, and then split them into heads\n",
    "        # 2. Apply scaled dot-product attention for each head\n",
    "        # 3. Concatenate the heads' outputs and apply the final linear projection\n",
    "\n",
    "        nbatches = Q.size(0)\n",
    "        # 1. Linearly project the queries, keys, and values, and then split them into heads\n",
    "\n",
    "        Q_proj = self.W_q(Q).view(Q.size(0), -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K_proj = self.W_k(K).view(K.size(0), -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V_proj = self.W_v(V).view(V.size(0), -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 2) Apply scaled dot-product attention for each head\n",
    "\n",
    "        # if mask is not None:\n",
    "        #   # Same mask applied to all h heads.\n",
    "        #   mask = mask.unsqueeze(1)\n",
    "\n",
    "        x = self.scaled_dot_product_attention(Q_proj, K_proj, V_proj, mask=mask)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.num_heads * self.d_k)\n",
    "        )\n",
    "\n",
    "        del Q\n",
    "        del K\n",
    "        del V\n",
    "\n",
    "        output = self.W_o(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b13867",
   "metadata": {
    "id": "70b13867"
   },
   "source": [
    "### Feed-Forward NN\n",
    "\n",
    "$$\n",
    "    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ab551",
   "metadata": {
    "id": "5d5ab551"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"The Positionwise Feedforward Network (FFN) module\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        x = self.dropout(self.relu(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034ef0a",
   "metadata": {
    "id": "4034ef0a"
   },
   "source": [
    "### Positional Encoding\n",
    "\n",
    "$$\n",
    "    \\text{PE}(pos, 2i) = \\sin(pos/10000^{2i/d_{\\text{model}}}) \\\\\n",
    "    \\text{PE}(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{\\text{model}}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca835c",
   "metadata": {
    "id": "bfca835c"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the positional encoding module using sinusoidal functions of different frequencies\n",
    "    for each dimension of the encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a positional encoding (PE) matrix with dimensions [max_seq_length, d_model].\n",
    "        # This matrix will contain the positional encodings for all possible positions up to max_seq_length.\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        # Generate a tensor of positions (0 to max_seq_length - 1) and reshape it to [max_seq_length, 1].\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Compute the division term used in the formulas for sin and cos functions.\n",
    "        # This term is based on the dimension of the model and the position, ensuring that the wavelengths\n",
    "        # form a geometric progression from 2π to 10000 * 2π. It uses only even indices for the dimensions.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply the sin function to even indices in the PE matrix. These values are determined by\n",
    "        # multiplying the position by the division term, creating a pattern where each position has\n",
    "        # a unique sinusoidal encoding.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # Apply the cos function to odd indices in the PE matrix, complementing the sin-encoded positions.\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register 'pe' as a buffer within the module. Unlike parameters, buffers are not updated during training.\n",
    "        # This is crucial because positional encodings are fixed and not subject to training updates.\n",
    "        # The unsqueeze(0) adds a batch dimension for easier broadcasting with input tensors.\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor x.\n",
    "        # x is expected to have dimensions [batch_size, seq_length, d_model].\n",
    "        # The positional encoding 'pe' is sliced to match the seq_length of 'x', and then added to 'x'.\n",
    "        # This operation leverages broadcasting to apply the same positional encoding across the batch.\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2475b",
   "metadata": {
    "id": "96e2475b"
   },
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000313c6",
   "metadata": {
    "id": "000313c6"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"An encoder layer consists of a multi-head self-attention sublayer and a feed forward sublayer,\n",
    "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        sublayer1_output = self.layer_norm1(x + self.dropout(self.self_attn(x, x, x, mask)))\n",
    "        sublayer2_output = self.layer_norm2(sublayer1_output + self.dropout(self.feed_forward(sublayer1_output)))\n",
    "        return sublayer2_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76215e02",
   "metadata": {
    "id": "76215e02"
   },
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27007e",
   "metadata": {
    "id": "ee27007e"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"A decoder layer consists of a multi-head self-attention, cross-attention and a feed-forward sublayers,\n",
    "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        # Apply self-attention mechanism with residual connection and layer normalization\n",
    "        sublayer1_output = self.layer_norm1(x + self.dropout(self.self_attn(x, x, x, tgt_mask)))\n",
    "\n",
    "        # Apply cross-attention mechanism with residual connection and layer normalization\n",
    "        sublayer2_output = self.layer_norm2(sublayer1_output + self.dropout(self.cross_attn(sublayer1_output, enc_output, enc_output, src_mask)))\n",
    "\n",
    "        # Apply feed forward network with residual connection and layer normalization\n",
    "        sublayer3_output = self.layer_norm3(sublayer2_output + self.dropout(self.feed_forward(sublayer2_output)))\n",
    "\n",
    "        return sublayer3_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad86614",
   "metadata": {
    "id": "2ad86614"
   },
   "source": [
    "### The Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af1f2a",
   "metadata": {
    "id": "31af1f2a"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, n_heads, d_ff, max_seq_length, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers for source and target\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Encoder and Decoder stacks\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
    "\n",
    "        # Output linear layer\n",
    "        self.out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialization\n",
    "        self.init_weights()\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize parameters with Glorot / fan_avg\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def create_source_mask(self, src):\n",
    "        \"\"\"Create masks for both padding tokens and future tokens\"\"\"\n",
    "        # Source padding mask\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_len]\n",
    "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
    "        # unsqueeze(2) adds a dimension for the attention scores\n",
    "        # This mask can be broadcasted across the src_len dimension of the attention scores,\n",
    "        # effectively masking out specific tokens across all heads and all positions in the sequence.\n",
    "        return src_mask\n",
    "\n",
    "    def create_target_mask(self, tgt):\n",
    "        # Target padding mask\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3).to(tgt.device)  # [batch_size, 1, tgt_len, 1]\n",
    "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
    "        # unsqueeze(3) adds a dimension for the attention scores\n",
    "        # The final shape allows the mask to be broadcast across the attention scores, ensuring positions only\n",
    "        # attend to allowed positions as dictated by the no-peak mask (the preceding positions) and the padding mask.\n",
    "\n",
    "        # Target no-peak mask\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_nopeak_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=tgt.device)).bool()\n",
    "\n",
    "        # Combine masks\n",
    "        tgt_mask = tgt_pad_mask & tgt_nopeak_mask  # [batch_size, 1, tgt_len, tgt_len]\n",
    "        return tgt_mask\n",
    "\n",
    "    def encode(self, src):\n",
    "        \"\"\"Encodes the source sequence using the Transformer encoder stack.\n",
    "        \"\"\"\n",
    "        src_mask = self.create_source_mask(src)\n",
    "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
    "\n",
    "        # Pass through each layer in the encoder\n",
    "        for layer in self.encoder:\n",
    "            src = layer(src, src_mask)\n",
    "        return src, src_mask\n",
    "\n",
    "    def decode(self, tgt, memory, src_mask):\n",
    "        \"\"\"Decodes the target sequence using the Transformer decoder stack, given the memory from the encoder.\n",
    "        \"\"\"\n",
    "        tgt_mask = self.create_target_mask(tgt)\n",
    "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
    "\n",
    "        # Pass through each layer in the decoder\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.out(tgt)\n",
    "        return output\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "\n",
    "        src_mask = self.create_source_mask(src)\n",
    "        tgt_mask = self.create_target_mask(tgt)\n",
    "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
    "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
    "\n",
    "        for layer in self.encoder:\n",
    "          src = layer(src, src_mask)\n",
    "        for layer in self.decoder:\n",
    "          tgt = layer(tgt, src, src_mask, tgt_mask)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.out(tgt)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3b60d",
   "metadata": {
    "id": "11a3b60d"
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the model\n",
    "src_vocab_size = 5000  # Size of source vocabulary\n",
    "tgt_vocab_size = 5000  # Size of target vocabulary\n",
    "d_model = 512          # Embedding dimension\n",
    "N = 6                  # Number of encoder and decoder layers\n",
    "num_heads = 8          # Number of attention heads\n",
    "d_ff = 2048            # Dimension of feed forward networks\n",
    "max_seq_length = 100   # Maximum sequence length\n",
    "dropout = 0.1          # Dropout rate\n",
    "pad_idx = 0            # Index of the padding token\n",
    "\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f7248",
   "metadata": {
    "id": "050f7248"
   },
   "source": [
    "### Testing on Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dadd1",
   "metadata": {
    "id": "149dadd1"
   },
   "outputs": [],
   "source": [
    "# Generate random sample data\n",
    "torch.manual_seed(42)\n",
    "\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b1c56",
   "metadata": {
    "id": "2d5b1c56"
   },
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45583975",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45583975",
    "outputId": "a004fcc4-1840-41ae-f353-978dea0f2174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([990], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the next token using the first token in the first target tensor\n",
    "model.eval()\n",
    "\n",
    "memory, src_mask = model.encode(src_data[:1, :])\n",
    "output = model.decode(tgt_data[:1, :1], memory, src_mask)\n",
    "y = output.view(-1, tgt_vocab_size).argmax(-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7e1c5",
   "metadata": {
    "id": "18b7e1c5"
   },
   "source": [
    "If your code is correct, you should get tensor([990])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OwEeNpNl3hic",
   "metadata": {
    "id": "OwEeNpNl3hic"
   },
   "source": [
    "Q3. Training the model: Train the model on a machine translation task from German to\n",
    "English using the Multi30k dataset (described in the notebook). Monitor the training\n",
    "and validation loss during training. Training the model on Google Colab (with GPU\n",
    "activated) for 20 epochs should take less than 30 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314148e",
   "metadata": {
    "id": "a314148e"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd8ff3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2afd8ff3",
    "outputId": "365e678a-1af9-4b8d-a6ff-eeaf34c83bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.604887008666992\n",
      "Epoch: 2, Loss: 8.500182151794434\n",
      "Epoch: 3, Loss: 8.3707275390625\n",
      "Epoch: 4, Loss: 8.295793533325195\n",
      "Epoch: 5, Loss: 8.237959861755371\n",
      "Epoch: 6, Loss: 8.191777229309082\n",
      "Epoch: 7, Loss: 8.164423942565918\n",
      "Epoch: 8, Loss: 8.141983985900879\n",
      "Epoch: 9, Loss: 8.12963581085205\n",
      "Epoch: 10, Loss: 8.121986389160156\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 10 epochs\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model.train()\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "#\n",
    "grad_clip = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(src_data, tgt_data[:, :-1])\n",
    "\n",
    "    # tgt_data is of shape [batch_size, tgt_len]\n",
    "    # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
    "    output = output.contiguous().view(-1, tgt_vocab_size)\n",
    "    tgt = tgt_data[:, 1:].contiguous().view(-1)\n",
    "    loss = criterion(output, tgt)\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f36f48",
   "metadata": {
    "id": "42f36f48"
   },
   "source": [
    "You should see the loss decreasing from around 8.6 to 8.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71caa7ed",
   "metadata": {
    "id": "71caa7ed"
   },
   "source": [
    "### Machine Translation Example\n",
    "\n",
    "Now we consider a real-world example using the Multi30k German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. <br>\n",
    "It is recommended to run this example on Google Colab, or on a machine with a strong GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e67f4",
   "metadata": {
    "id": "c38e67f4"
   },
   "source": [
    "#### Define Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f20abd",
   "metadata": {
    "id": "08f20abd"
   },
   "outputs": [],
   "source": [
    "# Load spacy models for tokenization\n",
    "try:\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "except IOError:\n",
    "    os.system(\"python -m spacy download de_core_news_sm\")\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "try:\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "except IOError:\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, language):\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample[language])\n",
    "\n",
    "tokenizer_de = get_tokenizer(tokenize_de)\n",
    "tokenizer_en = get_tokenizer(tokenize_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c9820",
   "metadata": {
    "id": "0e1c9820"
   },
   "source": [
    "#### Build Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474b5e6",
   "metadata": {
    "id": "7474b5e6"
   },
   "outputs": [],
   "source": [
    "train_data, _, _ = Multi30k(split=('train', 'valid', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C9_6a4jZEoZy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9_6a4jZEoZy",
    "outputId": "506de798-4f9e-4114-f9d9-6da35b4e922a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "vocab_src = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_de, 0),\n",
    "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab_tgt = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_en, 1),\n",
    "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "vocab_src.set_default_index(vocab_src['<unk>'])\n",
    "vocab_tgt.set_default_index(vocab_tgt['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ca38f",
   "metadata": {
    "id": "914ca38f"
   },
   "source": [
    "#### Create the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c862170",
   "metadata": {
    "id": "4c862170"
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the model\n",
    "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
    "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
    "d_model = 512  # Embedding dimension\n",
    "N = 6          # Number of encoder and decoder layers\n",
    "num_heads = 8  # Number of attention heads\n",
    "d_ff = 2048    # Dimension of feed forward networks\n",
    "max_seq_length = 5000 # Maximum sequence length\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Assume pad_idx is the padding index in the target vocabulary\n",
    "pad_idx = vocab_tgt['<pad>']\n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Hyperparameters for the training process\n",
    "batch_size = 128\n",
    "grad_clip = 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa58a6e",
   "metadata": {
    "id": "0aa58a6e"
   },
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca10d94",
   "metadata": {
    "id": "3ca10d94"
   },
   "outputs": [],
   "source": [
    "def data_process(raw_data_iter):\n",
    "    data = []\n",
    "    for raw_src, raw_tgt in raw_data_iter:\n",
    "        src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(raw_src)], dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor([vocab_tgt[token] for token in tokenizer_en(raw_tgt)], dtype=torch.long)\n",
    "        data.append((src_tensor, tgt_tensor))\n",
    "    return data\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k(split=('train', 'valid', 'test'))\n",
    "train_data = data_process(train_data)\n",
    "valid_data = data_process(valid_data)\n",
    "#test_data = data_process(test_data)\n",
    "# The test set of Multi30k is corrupted\n",
    "# See https://discuss.pytorch.org/t/unicodedecodeerror-when-running-test-iterator/192818/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f452a",
   "metadata": {
    "id": "1c2f452a"
   },
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    \"\"\"Processes a batch of source-target pairs by adding start-of-sequence (BOS) and end-of-sequence (EOS) tokens\n",
    "    to each sequence and padding all sequences to the same length.\n",
    "\n",
    "    Parameters:\n",
    "    - data_batch (Iterable[Tuple[Tensor, Tensor]]): A batch of source-target pairs, where each element is a tuple\n",
    "      containing the source sequence tensor and the target sequence tensor.\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = [], []\n",
    "    src_batch, tgt_batch = [], []\n",
    "\n",
    "    # Iterate over each source-target pair in the provided batch\n",
    "    for src_item, tgt_item in data_batch:\n",
    "        # Prepend the start-of-sequence (BOS) token and append the end-of-sequence (EOS) token to the sequences\n",
    "        src_batch.append(torch.cat([torch.tensor([vocab_src['<bos>']]), src_item,\n",
    "                                    torch.tensor([vocab_src['<eos>']])], dim=0))\n",
    "        tgt_batch.append(torch.cat([torch.tensor([vocab_tgt['<bos>']]), tgt_item,\n",
    "                                    torch.tensor([vocab_tgt['<eos>']])], dim=0))\n",
    "\n",
    "    # Pad the sequences in the source batch to ensure they all have the same length.\n",
    "    # 'batch_first=True' indicates that the batch dimension should come first in the resulting tensor.\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_tgt['<pad>'], batch_first=True)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
    "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
    "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "# Similarly, DataLoader for the validation data\n",
    "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd8431",
   "metadata": {
    "id": "2cbd8431"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, grad_clip):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch over the given dataset.\n",
    "    This function iterates over the provided data iterator, performing the forward and backward passes for each batch.\n",
    "    It employs teacher forcing by feeding the shifted target sequence (excluding the last token) as input to the decoder.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model to be trained.\n",
    "    - iterator (Iterable): An iterable object that returns batches of data.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to use for updating the model parameters.\n",
    "    - criterion (Callable): The loss function used to compute the difference between the model's predictions and the actual targets.\n",
    "    - grad_clip (float): The maximum norm of the gradients for gradient clipping.\n",
    "\n",
    "    Returns:\n",
    "    - float: The average loss for the epoch, computed as the total loss over all batches divided by the number of batches in the iterator.\n",
    "    \"\"\"\n",
    "    # Set the model to training mode.\n",
    "    # This enables dropout, layer normalization etc., which behave differently during training.\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Enumerate over the data iterator to get batches\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # Unpack the batch to get source (src) and target (tgt) sequences\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model.\n",
    "        # For seq2seq models, the decoder input (tgt[:, :-1]) excludes the last token, implementing teacher forcing.\n",
    "        output = model(src, tgt[:, :-1])\n",
    "\n",
    "        # Reshape the output and target tensors to compute loss.\n",
    "        # The output tensor is reshaped to a 2D tensor where rows correspond to each token in the batch and columns to vocabulary size.\n",
    "\n",
    "        # tgt is of shape [batch_size, tgt_len]\n",
    "        # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
    "        output = output.contiguous().view(-1, tgt_vocab_size)\n",
    "\n",
    "        # The target tensor is reshaped to a 1D tensor, excluding the first token (BOS) from each sequence.\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        # Compute loss, perform backpropagation, and update model parameters\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Compute average loss per batch for the current epoch\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d18844",
   "metadata": {
    "id": "31d18844"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a given dataset.\n",
    "    This function is similar to the training loop, but without the backward pass and parameter updates. I\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287e5dd",
   "metadata": {
    "id": "0287e5dd"
   },
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02463b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d02463b8",
    "outputId": "c66cd349-c2e6-4fa1-a9dc-d6c6a6bf09f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "\tTrain Loss: 5.692\n",
      "\tVal Loss: 5.009\n",
      "\n",
      "Epoch: 2\n",
      "\tTrain Loss: 4.880\n",
      "\tVal Loss: 4.806\n",
      "\n",
      "Epoch: 3\n",
      "\tTrain Loss: 4.701\n",
      "\tVal Loss: 4.636\n",
      "\n",
      "Epoch: 4\n",
      "\tTrain Loss: 4.471\n",
      "\tVal Loss: 4.307\n",
      "\n",
      "Epoch: 5\n",
      "\tTrain Loss: 4.136\n",
      "\tVal Loss: 4.037\n",
      "\n",
      "Epoch: 6\n",
      "\tTrain Loss: 3.910\n",
      "\tVal Loss: 3.896\n",
      "\n",
      "Epoch: 7\n",
      "\tTrain Loss: 3.761\n",
      "\tVal Loss: 3.752\n",
      "\n",
      "Epoch: 8\n",
      "\tTrain Loss: 3.630\n",
      "\tVal Loss: 3.666\n",
      "\n",
      "Epoch: 9\n",
      "\tTrain Loss: 3.529\n",
      "\tVal Loss: 3.609\n",
      "\n",
      "Epoch: 10\n",
      "\tTrain Loss: 3.451\n",
      "\tVal Loss: 3.563\n",
      "\n",
      "Epoch: 11\n",
      "\tTrain Loss: 3.383\n",
      "\tVal Loss: 3.512\n",
      "\n",
      "Epoch: 12\n",
      "\tTrain Loss: 3.318\n",
      "\tVal Loss: 3.477\n",
      "\n",
      "Epoch: 13\n",
      "\tTrain Loss: 3.254\n",
      "\tVal Loss: 3.424\n",
      "\n",
      "Epoch: 14\n",
      "\tTrain Loss: 3.190\n",
      "\tVal Loss: 3.374\n",
      "\n",
      "Epoch: 15\n",
      "\tTrain Loss: 3.130\n",
      "\tVal Loss: 3.341\n",
      "\n",
      "Epoch: 16\n",
      "\tTrain Loss: 3.075\n",
      "\tVal Loss: 3.284\n",
      "\n",
      "Epoch: 17\n",
      "\tTrain Loss: 3.021\n",
      "\tVal Loss: 3.254\n",
      "\n",
      "Epoch: 18\n",
      "\tTrain Loss: 2.969\n",
      "\tVal Loss: 3.225\n",
      "\n",
      "Epoch: 19\n",
      "\tTrain Loss: 2.919\n",
      "\tVal Loss: 3.198\n",
      "\n",
      "Epoch: 20\n",
      "\tTrain Loss: 2.871\n",
      "\tVal Loss: 3.175\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
    "    val_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    print(f'\\nEpoch: {epoch + 1}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tVal Loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a844bb",
   "metadata": {
    "id": "64a844bb"
   },
   "source": [
    "The train loss should decrease from around 5.7 to 2.8 after 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a7300",
   "metadata": {
    "id": "817a7300"
   },
   "source": [
    "#### Translating a Sample Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WxnY4LhBLpsx",
   "metadata": {
    "id": "WxnY4LhBLpsx"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model.pth')\n",
    "# model saved as model_20_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5gip-UdbPFkc",
   "metadata": {
    "id": "5gip-UdbPFkc"
   },
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5wyCrwWPGAB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "c5wyCrwWPGAB",
    "outputId": "09fe3c01-3841-43a1-fc81-69645f13c47b"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_f5a3f9f8-aab3-4fcb-96f6-64a2c97d99b7\", \"model.pth\", 270651302)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# files.download('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J5NWM856PVZV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J5NWM856PVZV",
    "outputId": "42a5d039-b83a-4bde-a2b9-c3d8190d31f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1Eb8u3_HPqJD",
   "metadata": {
    "id": "1Eb8u3_HPqJD"
   },
   "outputs": [],
   "source": [
    "# %cp /content/model.pth /content/gdrive/My\\ Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hB3JJ0QiLtBy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hB3JJ0QiLtBy",
    "outputId": "c58d6d3b-b22d-4de1-aecd-e427f9b9db84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the hyperparameters of the model\n",
    "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
    "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
    "d_model = 512  # Embedding dimension\n",
    "N = 6          # Number of encoder and decoder layers\n",
    "num_heads = 8  # Number of attention heads\n",
    "d_ff = 2048    # Dimension of feed forward networks\n",
    "max_seq_length = 5000 # Maximum sequence length\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
    "model.load_state_dict(torch.load('/content/gdrive/MyDrive/LLM-HW2-Models/model_20_epoch.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDy9wCEP4N1p",
   "metadata": {
    "id": "tDy9wCEP4N1p"
   },
   "source": [
    "Q4 (a) Implement a function for translating sentences using greedy decoding. Use it to translate several sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1bd36c",
   "metadata": {
    "id": "0b1bd36c"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, vocab_src, vocab_tgt, max_length=50):\n",
    "    \"\"\"\n",
    "    Translates a given source sentence into the target language using a trained Transformer model.\n",
    "    The function preprocesses the input sentence by tokenizing and converting it to tensor format, then uses the model's\n",
    "    encode and decode methods to generate the translated sentence. The translation process is performed token by token\n",
    "    using greedy decoding, selecting the most likely next token at each step until an <eos> token is produced or the\n",
    "    maximum length is reached.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The trained Transformer model.\n",
    "    - sentence (str): The source sentence to translate.\n",
    "    - vocab_src (dict): The source vocabulary mapping of tokens to indices. It should include special tokens such as\n",
    "      '<bos>' (beginning of sentence) and '<eos>' (end of sentence).\n",
    "    - vocab_tgt (dict): The target vocabulary mapping of indices to tokens. It should provide a method `lookup_token`\n",
    "      to convert token indices back to the string representation.\n",
    "    - max_length (int, optional): The maximum allowed length for the generated translation. The decoding process will\n",
    "      stop when this length is reached if an <eos> token has not yet been generated.\n",
    "\n",
    "    Returns:\n",
    "    - str: The translated sentence as a string of text in the target language.\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "        # Tokenize the input sentence and convert it to tensor\n",
    "    tokens = [vocab_src[token] for token in sentence.split()]\n",
    "    tokens = [vocab_src['<bos>']] + tokens + [vocab_src['<eos>']]  # Add <bos> and <eos> tokens\n",
    "    src_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Encode the source sentence\n",
    "    src_encoded, src_mask = model.encode(src_tensor)\n",
    "\n",
    "    # Initialize the generated translation as an empty list\n",
    "    translated_sentence = []\n",
    "\n",
    "    # Initialize the input token as the <bos> token index\n",
    "    tgt_token = torch.tensor([[vocab_tgt['<bos>']]], dtype=torch.long).to(src_tensor.device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Generate a subsequent mask for the target tensor\n",
    "        tgt_mask = model.create_target_mask(tgt_token)\n",
    "\n",
    "        # Pass the target tensor and encoded source representation through the model to get the decoded output\n",
    "        output = model.decode(tgt_token, src_encoded, src_mask)\n",
    "\n",
    "        # Select the token with the highest probability from the last timestep\n",
    "        pred_token = output.argmax(dim=-1)[:, -1].item()\n",
    "\n",
    "        # If the predicted token is the <eos> token, stop generating\n",
    "        if pred_token == vocab_tgt['<eos>']:\n",
    "            break\n",
    "\n",
    "        # Append the predicted token to the translated sentence\n",
    "        translated_sentence.append(vocab_tgt.lookup_token(pred_token))\n",
    "\n",
    "        # Append the predicted token to the target tensor for the next timestep\n",
    "        tgt_token = torch.cat((tgt_token, torch.tensor([[pred_token]], dtype=torch.long).to(model.tgt_embedding.weight.device)), dim=1)  # Move to model's device\n",
    "\n",
    "    # Convert the translated sentence list to a string\n",
    "    translated_sentence = ' '.join(translated_sentence)\n",
    "\n",
    "    return translated_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438aff1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "438aff1f",
    "outputId": "bbe3b095-6341-4ff2-b38b-27f1f736e79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: A young boy is playing with a toy .\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
    "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
    "print(f'Translated sentence: {translated_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6be352",
   "metadata": {
    "id": "ec6be352"
   },
   "source": [
    "You should get a translation similar to the reference after 20 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OVpLZXH4OL1N",
   "metadata": {
    "id": "OVpLZXH4OL1N"
   },
   "outputs": [],
   "source": [
    "# Können Sie mir helfen?          Can you help me?\n",
    "# Ich spreche ein wenig Deutsch. \tI speak a little German.\n",
    "# Ich komme aus den USA.           I’m from the United States\n",
    "# Freut mich, Sie kennenzulernen.  It’s a pleasure to meet you\n",
    "# Ich möchte ein Bier.            I’d like a beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C189sr_4Yjvq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C189sr_4Yjvq",
    "outputId": "d7c82b96-070f-4b62-cdc0-7f6293bf63dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: A young boy is playing with a toy . Correct Translation: A little boy playing outside with a ball.\n",
      "Translated sentence: A group of people are playing instruments . Correct Translation: Can you help me?\n",
      "Translated sentence: A crowd of people at a party . Correct Translation: I speak a little German.\n",
      "Translated sentence: A crowd of people are walking in a city . Correct Translation: I’m from the United States\n",
      "Translated sentence: A crowd of people are playing . Correct Translation: It’s a pleasure to meet you.\n",
      "Translated sentence: A group of people are playing . Correct Translation: I’d like a beer.\n"
     ]
    }
   ],
   "source": [
    "german_sentences = [\"Ein kleiner Junge spielt draußen mit einem Ball.\",\"Können Sie mir helfen?\",\"Ich spreche ein wenig Deutsch.\",\"Ich komme aus den USA.\",\"Freut mich, Sie kennenzulernen.\",\"Ich möchte ein Bier.\"]\n",
    "english_translation = [\"A little boy playing outside with a ball.\",\"Can you help me?\",\"I speak a little German.\",\"I’m from the United States\",\"It’s a pleasure to meet you.\",\"I’d like a beer.\"]\n",
    "\n",
    "for idx in range(len(german_sentences)):\n",
    "  translated_sentence = translate_sentence(model, german_sentences[idx], vocab_src, vocab_tgt)\n",
    "  print(f'Translated sentence: {translated_sentence} Correct Translation: {english_translation[idx]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nE1snMTbaO4Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nE1snMTbaO4Y",
    "outputId": "32a16f0f-5b45-4e0a-cf92-bb723097cd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: A crowd of people are playing .\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"Mein Name ist\"  # German for \"A little boy playing outside with a ball.\"\n",
    "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
    "print(f'Translated sentence: {translated_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uu_VC3qS4ZIv",
   "metadata": {
    "id": "Uu_VC3qS4ZIv"
   },
   "source": [
    "Q6 Bonus: Implement beam search instead of greedy decoding for better-quality translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VuKggjIHcE29",
   "metadata": {
    "id": "VuKggjIHcE29"
   },
   "outputs": [],
   "source": [
    "def translate_sentence_beam_search(model, sentence, vocab_src, vocab_tgt, max_length=50, beam_size=2):\n",
    "    # Tokenize the input sentence and convert it to tensor\n",
    "    tokens = [vocab_src[token] for token in sentence.split()]\n",
    "    tokens = [vocab_src['<bos>']] + tokens + [vocab_src['<eos>']]  # Add <bos> and <eos> tokens\n",
    "    src_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Encode the source sentence\n",
    "    src_encoded, src_mask = model.encode(src_tensor)\n",
    "\n",
    "    # Initialize the list of partial hypotheses with the initial hypothesis containing only the <bos> token\n",
    "    hypotheses = [([vocab_tgt['<bos>']], 1.0)]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        next_hypotheses = []\n",
    "\n",
    "        # For each partial hypothesis\n",
    "        for hypothesis in hypotheses:\n",
    "            tgt_token = torch.tensor([[hypothesis[0][-1]]], dtype=torch.long).to(src_tensor.device)\n",
    "\n",
    "            # Generate a subsequent mask for the target tensor\n",
    "            tgt_mask = model.create_target_mask(tgt_token)\n",
    "\n",
    "            # Pass the target tensor and encoded source representation through the model to get the decoded output\n",
    "            output = model.decode(tgt_token, src_encoded, src_mask)\n",
    "\n",
    "            # Get the top-k predicted tokens and their probabilities\n",
    "            top_k_probabilities, top_k_indices = torch.topk(torch.softmax(output[:, -1, :], dim=-1), k=beam_size)\n",
    "\n",
    "            # For each predicted token in the top-k list\n",
    "            for k in range(beam_size):\n",
    "                next_token_index = top_k_indices[0][k].item()\n",
    "                next_token_prob = top_k_probabilities[0][k].item()\n",
    "\n",
    "                # Update the new hypothesis by appending the next token and multiplying the probability\n",
    "                new_hypothesis = (hypothesis[0] + [next_token_index], hypothesis[1] * next_token_prob)\n",
    "\n",
    "                # Append the new hypothesis to the list of next hypotheses\n",
    "                next_hypotheses.append(new_hypothesis)\n",
    "\n",
    "        # Sort the next hypotheses based on their probabilities and keep only the top beam_size hypotheses\n",
    "        next_hypotheses.sort(key=lambda x: x[1], reverse=True)\n",
    "        hypotheses = next_hypotheses[:beam_size]\n",
    "\n",
    "        # Check if any hypothesis ends with the <eos> token, if so, break the loop\n",
    "        if any([hypothesis[0][-1] == vocab_tgt['<eos>'] for hypothesis in hypotheses]):\n",
    "            break\n",
    "\n",
    "    # Select the hypothesis with the highest probability as the final translation\n",
    "    best_translation = max(hypotheses, key=lambda x: x[1])[0]\n",
    "\n",
    "    # Convert the translated sentence list to a string\n",
    "    translated_sentence = ' '.join([vocab_tgt.lookup_token(token) for token in best_translation if token != vocab_tgt['<bos>'] and token != vocab_tgt['<eos>']])\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0T7h88nAdgjO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0T7h88nAdgjO",
    "outputId": "3c291371-f919-42c0-eaf5-2567c904dffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: Some A crowd A People .\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"Ich spreche ein wenig Deutsch.\"  # German for \"A little boy playing outside with a ball.\"\n",
    "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt)\n",
    "print(f'Translated sentence: {translated_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uTPCFtPndmWy",
   "metadata": {
    "id": "uTPCFtPndmWy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
